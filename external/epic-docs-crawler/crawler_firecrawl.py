#!/usr/bin/env python3
"""
Epic Games UEFN Documentation Crawler - Firecrawl ç‰ˆæœ¬
ä½¿ç”¨ Firecrawl API æŠ“å–å— Cloudflare ä¿æŠ¤çš„ Epic æ–‡æ¡£

Firecrawl æ˜¯ä¸“ä¸šçš„ç½‘é¡µæŠ“å–æœåŠ¡ï¼Œèƒ½å¤Ÿå¤„ç† JavaScript æ¸²æŸ“å’Œ Cloudflare ä¿æŠ¤ã€‚
å…è´¹é¢åº¦: 500 credits/month

è·å– API Key: https://www.firecrawl.dev/

Usage:
    export FIRECRAWL_API_KEY="your-api-key"
    python crawler_firecrawl.py                    # æŠ“å–æ ¸å¿ƒæ–‡æ¡£
    python crawler_firecrawl.py --full             # æŠ“å–å…¨éƒ¨æ–‡æ¡£
    python crawler_firecrawl.py --url <url>        # æŠ“å–æŒ‡å®šé¡µé¢
"""

import os
import sys
import json
import hashlib
import argparse
from pathlib import Path
from datetime import datetime

try:
    from firecrawl import FirecrawlApp
except ImportError:
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install firecrawl-py")
    sys.exit(1)


# ============== é…ç½® ==============

BASE_URL = "https://dev.epicgames.com/documentation/en-us/uefn"

# æ ¸å¿ƒæ–‡æ¡£ URLs
CORE_DOCS = [
    "/verse-language-reference",
    "/verse-api", 
    "/verse-language-quick-reference",
    "/specifiers-and-attributes-in-verse",
    "/modules-and-paths-in-verse",
    "/option-values-in-verse",
    "/failure-and-failable-expressions-in-verse",
    "/concurrency-in-verse",
    "/concurrency-overview-in-verse",
    "/classes-and-objects-in-verse",
    "/interfaces-in-verse",
    "/structs-in-verse",
    "/array-in-verse",
    "/map-in-verse",
    "/types-in-verse",
]

# æ‰©å±•æ–‡æ¡£
EXTENDED_DOCS = [
    "/devices",
    "/using-devices-in-verse",
    "/creative-devices-in-verse",
    "/custom-ui-in-verse",
    "/creating-custom-ui",
    "/characters-in-verse",
    "/players-in-verse",
    "/teams-in-verse",
    "/props-in-verse",
    "/verse-style-guide",
    "/debugging-verse-code",
    "/verse-best-practices",
]

OUTPUT_DIR = Path(__file__).parent.parent.parent.parent / "i18n/zh/skills/uefn-dev/references/official-docs"


class FirecrawlDocsCrawler:
    """ä½¿ç”¨ Firecrawl API æŠ“å– Epic æ–‡æ¡£"""
    
    def __init__(self, api_key: str = None, output_dir: Path = OUTPUT_DIR):
        self.api_key = api_key or os.environ.get("FIRECRAWL_API_KEY")
        if not self.api_key:
            print("âŒ è¯·è®¾ç½® FIRECRAWL_API_KEY ç¯å¢ƒå˜é‡")
            print("   è·å– API Key: https://www.firecrawl.dev/")
            sys.exit(1)
        
        self.app = FirecrawlApp(api_key=self.api_key)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.index = {}
        self.failed_urls = []
        
        # åŠ è½½ç´¢å¼•
        self.index_file = self.output_dir / "index.json"
        if self.index_file.exists():
            with open(self.index_file, "r", encoding="utf-8") as f:
                self.index = json.load(f)
    
    def _url_to_filename(self, url: str) -> str:
        """URL è½¬æ–‡ä»¶å"""
        path = url.replace(BASE_URL, '').strip('/')
        if not path:
            return "index"
        return path.replace('/', '_').replace('-', '_')
    
    def _save_content(self, url: str, content: str, title: str = ""):
        """ä¿å­˜æŠ“å–çš„å†…å®¹"""
        filename = self._url_to_filename(url)
        filepath = self.output_dir / f"{filename}.md"
        
        markdown = f"""---
title: "{title or filename}"
source: "{url}"
crawled_at: "{datetime.now().isoformat()}"
---

{content}
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        self.index[url] = {
            "title": title,
            "file": filepath.name,
            "crawled_at": datetime.now().isoformat(),
            "size": len(content),
        }
        
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, indent=2, ensure_ascii=False)
        
        print(f"  ğŸ’¾ å·²ä¿å­˜: {filepath.name} ({len(content)/1024:.1f} KB)")
    
    def crawl_url(self, url: str) -> bool:
        """æŠ“å–å•ä¸ª URL"""
        full_url = url if url.startswith('http') else BASE_URL + url
        print(f"\nğŸ“„ æ­£åœ¨æŠ“å–: {full_url}")
        
        try:
            # ä½¿ç”¨ Firecrawl æŠ“å–
            result = self.app.scrape_url(
                full_url,
                params={
                    'formats': ['markdown'],
                    'waitFor': 5000,  # ç­‰å¾… JS æ¸²æŸ“
                }
            )
            
            if result and result.get('markdown'):
                content = result['markdown']
                title = result.get('metadata', {}).get('title', '')
                self._save_content(full_url, content, title)
                return True
            else:
                print(f"  âŒ æœªè·å–åˆ°å†…å®¹")
                self.failed_urls.append(full_url)
                return False
                
        except Exception as e:
            print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
            self.failed_urls.append(full_url)
            return False
    
    def crawl_site(self, limit: int = 50):
        """ä½¿ç”¨ Firecrawl çš„ crawl åŠŸèƒ½æŠ“å–æ•´ä¸ªç«™ç‚¹"""
        print(f"\nğŸŒ å¼€å§‹æŠ“å–æ•´ä¸ª UEFN æ–‡æ¡£ç«™ç‚¹ (æœ€å¤š {limit} é¡µ)")
        
        try:
            result = self.app.crawl_url(
                BASE_URL,
                params={
                    'limit': limit,
                    'scrapeOptions': {
                        'formats': ['markdown'],
                    }
                },
                poll_interval=5
            )
            
            if result and result.get('data'):
                for page in result['data']:
                    url = page.get('metadata', {}).get('sourceURL', '')
                    content = page.get('markdown', '')
                    title = page.get('metadata', {}).get('title', '')
                    if content:
                        self._save_content(url, content, title)
                
                print(f"\nâœ… æŠ“å–å®Œæˆ: {len(result['data'])} é¡µ")
            else:
                print("âŒ æœªè·å–åˆ°æ•°æ®")
                
        except Exception as e:
            print(f"âŒ ç«™ç‚¹æŠ“å–å¤±è´¥: {e}")
    
    def crawl_core(self):
        """æŠ“å–æ ¸å¿ƒæ–‡æ¡£"""
        print("\n" + "="*60)
        print("ğŸ¯ ä½¿ç”¨ Firecrawl æŠ“å–æ ¸å¿ƒ UEFN/Verse æ–‡æ¡£")
        print("="*60)
        
        success = 0
        for doc_path in CORE_DOCS:
            if self.crawl_url(doc_path):
                success += 1
        
        self._print_summary(success, len(CORE_DOCS))
    
    def crawl_full(self):
        """æŠ“å–å…¨éƒ¨æ–‡æ¡£"""
        print("\n" + "="*60)
        print("ğŸ¯ ä½¿ç”¨ Firecrawl æŠ“å–å…¨éƒ¨ UEFN/Verse æ–‡æ¡£")
        print("="*60)
        
        all_docs = CORE_DOCS + EXTENDED_DOCS
        success = 0
        for doc_path in all_docs:
            if self.crawl_url(doc_path):
                success += 1
        
        self._print_summary(success, len(all_docs))
    
    def _print_summary(self, success: int, total: int):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æŠ“å–å®Œæˆ")
        print("="*60)
        print(f"  âœ… æˆåŠŸ: {success}/{total}")
        print(f"  âŒ å¤±è´¥: {len(self.failed_urls)}")
        print(f"  ğŸ“ è¾“å‡º: {self.output_dir}")
        
        if self.failed_urls:
            print("\nâŒ å¤±è´¥çš„ URLs:")
            for url in self.failed_urls:
                print(f"    - {url}")


def main():
    parser = argparse.ArgumentParser(description="Epic UEFN æ–‡æ¡£çˆ¬è™« (Firecrawl)")
    parser.add_argument('--full', action='store_true', help='æŠ“å–å…¨éƒ¨æ–‡æ¡£')
    parser.add_argument('--url', type=str, help='æŠ“å–æŒ‡å®š URL')
    parser.add_argument('--crawl-site', type=int, metavar='LIMIT', help='æŠ“å–æ•´ä¸ªç«™ç‚¹')
    parser.add_argument('--api-key', type=str, help='Firecrawl API Key')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    output_dir = Path(args.output) if args.output else OUTPUT_DIR
    crawler = FirecrawlDocsCrawler(api_key=args.api_key, output_dir=output_dir)
    
    if args.crawl_site:
        crawler.crawl_site(limit=args.crawl_site)
    elif args.url:
        crawler.crawl_url(args.url)
    elif args.full:
        crawler.crawl_full()
    else:
        crawler.crawl_core()


if __name__ == "__main__":
    main()
