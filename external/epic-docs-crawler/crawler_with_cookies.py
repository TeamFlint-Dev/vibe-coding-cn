#!/usr/bin/env python3
"""
Epic Games UEFN Documentation Crawler - Cookie è¾…åŠ©ç‰ˆæœ¬

æ­¤ç‰ˆæœ¬éœ€è¦ç”¨æˆ·å…ˆåœ¨æµè§ˆå™¨ä¸­è®¿é—® Epic æ–‡æ¡£ï¼Œé€šè¿‡ Cloudflare éªŒè¯åå¯¼å‡º cookiesã€‚
ç„¶åçˆ¬è™«ä½¿ç”¨è¿™äº› cookies æ¥æŠ“å–æ–‡æ¡£ã€‚

æ­¥éª¤:
1. åœ¨ Chrome ä¸­æ‰“å¼€: https://dev.epicgames.com/documentation/en-us/uefn/verse-api
2. å®Œæˆ Cloudflare éªŒè¯
3. å®‰è£… "EditThisCookie" æˆ– "Cookie-Editor" æ‰©å±•
4. å¯¼å‡º cookies ä¸º JSON æ ¼å¼ï¼Œä¿å­˜åˆ° cookies.json
5. è¿è¡Œæ­¤è„šæœ¬: python crawler_with_cookies.py

Usage:
    python crawler_with_cookies.py --cookies cookies.json
    python crawler_with_cookies.py --cookies cookies.json --full
"""

import os
import sys
import json
import time
import hashlib
import argparse
import asyncio
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

try:
    from playwright.async_api import async_playwright
    from markdownify import markdownify as md
except ImportError:
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install playwright markdownify")
    sys.exit(1)


BASE_URL = "https://dev.epicgames.com/documentation/en-us/uefn"

CORE_DOCS = [
    "/verse-language-reference",
    "/verse-api",
    "/verse-language-quick-reference",
    "/specifiers-and-attributes-in-verse",
    "/modules-and-paths-in-verse",
    "/option-values-in-verse",
    "/failure-and-failable-expressions-in-verse",
    "/concurrency-in-verse",
    "/classes-and-objects-in-verse",
    "/interfaces-in-verse",
    "/structs-in-verse",
    "/array-in-verse",
    "/map-in-verse",
    "/types-in-verse",
]

EXTENDED_DOCS = [
    "/devices",
    "/using-devices-in-verse", 
    "/custom-ui-in-verse",
    "/characters-in-verse",
    "/players-in-verse",
    "/verse-style-guide",
    "/debugging-verse-code",
]

OUTPUT_DIR = Path(__file__).parent.parent.parent.parent / "i18n/zh/skills/uefn-dev/references/official-docs"


class CookieCrawler:
    """ä½¿ç”¨é¢„è®¾ cookies çš„çˆ¬è™«"""
    
    def __init__(self, cookies_file: str, output_dir: Path = OUTPUT_DIR):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.cookies = self._load_cookies(cookies_file)
        self.browser = None
        self.context = None
        self.page = None
        self.playwright = None
        self.crawled_urls = set()
        self.failed_urls = []
        self.index = {}
        
        self.index_file = self.output_dir / "index.json"
        if self.index_file.exists():
            with open(self.index_file, "r", encoding="utf-8") as f:
                self.index = json.load(f)
    
    def _load_cookies(self, cookies_file: str) -> list:
        """åŠ è½½ cookies æ–‡ä»¶"""
        if not os.path.exists(cookies_file):
            print(f"âŒ Cookies æ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}")
            print("\nğŸ“‹ è·å– cookies çš„æ­¥éª¤:")
            print("1. åœ¨ Chrome ä¸­æ‰“å¼€: https://dev.epicgames.com/documentation/en-us/uefn/verse-api")
            print("2. å®Œæˆ Cloudflare éªŒè¯ï¼ˆå¦‚æœ‰ï¼‰")
            print("3. å®‰è£… 'EditThisCookie' æˆ– 'Cookie-Editor' Chrome æ‰©å±•")
            print("4. ç‚¹å‡»æ‰©å±•å›¾æ ‡ -> å¯¼å‡º -> é€‰æ‹© JSON æ ¼å¼")
            print("5. ä¿å­˜ä¸º cookies.json")
            print(f"6. è¿è¡Œ: python {__file__} --cookies cookies.json")
            sys.exit(1)
        
        with open(cookies_file, 'r', encoding='utf-8') as f:
            cookies = json.load(f)
        
        # è½¬æ¢ä¸º Playwright æ ¼å¼
        playwright_cookies = []
        for cookie in cookies:
            pc = {
                'name': cookie.get('name'),
                'value': cookie.get('value'),
                'domain': cookie.get('domain', '.dev.epicgames.com'),
                'path': cookie.get('path', '/'),
            }
            if cookie.get('expirationDate'):
                pc['expires'] = cookie['expirationDate']
            if cookie.get('secure'):
                pc['secure'] = cookie['secure']
            if cookie.get('httpOnly'):
                pc['httpOnly'] = cookie['httpOnly']
            if cookie.get('sameSite'):
                pc['sameSite'] = cookie['sameSite'].capitalize()
            playwright_cookies.append(pc)
        
        print(f"âœ… å·²åŠ è½½ {len(playwright_cookies)} ä¸ª cookies")
        return playwright_cookies
    
    async def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨å¹¶æ³¨å…¥ cookies"""
        self.playwright = await async_playwright().start()
        
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # ä½¿ç”¨ç”¨æˆ·çš„ç²¾ç¡®æµè§ˆå™¨æŒ‡çº¹
        self.context = await self.browser.new_context(
            viewport={'width': 2560, 'height': 1440},
            device_scale_factor=0.9,
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            locale='zh-CN',
            timezone_id='Asia/Shanghai',
            color_scheme='light',
        )
        
        # æ³¨å…¥ cookies
        await self.context.add_cookies(self.cookies)
        
        self.page = await self.context.new_page()
        
        # è®¾ç½®é¢å¤–çš„æµè§ˆå™¨å±æ€§
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'platform', { get: () => 'Win32' });
            Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });
            Object.defineProperty(screen, 'colorDepth', { get: () => 24 });
        """)
        
        print("ğŸŒ æµè§ˆå™¨å·²å¯åŠ¨ï¼Œcookies å·²æ³¨å…¥")
    
    async def _extract_content(self) -> dict:
        """æå–é¡µé¢å†…å®¹"""
        result = {"title": "", "content": ""}
        
        try:
            await self.page.wait_for_selector('article, main', timeout=10000)
        except:
            pass
        
        try:
            h1 = await self.page.query_selector('h1')
            if h1:
                result["title"] = await h1.inner_text()
        except:
            result["title"] = await self.page.title() or "Untitled"
        
        for selector in ['article', 'main article', '.documentation-content', 'main']:
            try:
                el = await self.page.query_selector(selector)
                if el:
                    html = await el.inner_html()
                    result["content"] = md(html, heading_style="atx", code_language="verse")
                    if len(result["content"]) > 200:
                        break
            except:
                continue
        
        return result
    
    def _save_content(self, url: str, content: dict):
        """ä¿å­˜å†…å®¹"""
        path = url.replace(BASE_URL, '').strip('/')
        filename = (path or 'index').replace('/', '_').replace('-', '_')
        filepath = self.output_dir / f"{filename}.md"
        
        markdown = f"""---
title: "{content['title']}"
source: "{url}"
crawled_at: "{datetime.now().isoformat()}"
---

# {content['title']}

{content['content']}
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        self.index[url] = {
            "title": content["title"],
            "file": filepath.name,
            "size": len(content["content"]),
        }
        
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, indent=2, ensure_ascii=False)
        
        print(f"  ğŸ’¾ å·²ä¿å­˜: {filepath.name} ({len(content['content'])/1024:.1f} KB)")
    
    async def crawl_url(self, url: str) -> bool:
        """æŠ“å–å•ä¸ª URL"""
        full_url = url if url.startswith('http') else BASE_URL + url
        print(f"\nğŸ“„ æ­£åœ¨æŠ“å–: {full_url}")
        
        try:
            response = await self.page.goto(full_url, wait_until='domcontentloaded', timeout=60000)
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(3)
            
            if response and response.status == 403:
                print("  âŒ 403 Forbidden - Cookies å¯èƒ½å·²è¿‡æœŸ")
                self.failed_urls.append(full_url)
                return False
            
            await asyncio.sleep(2)
            
            # æ£€æŸ¥æ˜¯å¦ä»åœ¨ Cloudflare é¡µé¢
            title = await self.page.title()
            if "just a moment" in (title or "").lower():
                print("  âŒ ä»è¢« Cloudflare æ‹¦æˆªï¼Œè¯·æ›´æ–° cookies")
                self.failed_urls.append(full_url)
                return False
            
            content = await self._extract_content()
            
            if len(content.get("content", "")) < 100:
                print("  âŒ å†…å®¹è¿‡å°‘")
                self.failed_urls.append(full_url)
                return False
            
            self._save_content(full_url, content)
            self.crawled_urls.add(full_url)
            
            await asyncio.sleep(3)
            return True
            
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            self.failed_urls.append(full_url)
            return False
    
    async def crawl_core(self):
        """æŠ“å–æ ¸å¿ƒæ–‡æ¡£"""
        print("\n" + "="*60)
        print("ğŸ¯ ä½¿ç”¨ Cookies æŠ“å–æ ¸å¿ƒ UEFN/Verse æ–‡æ¡£")
        print("="*60)
        
        await self._init_browser()
        
        success = 0
        for doc in CORE_DOCS:
            if await self.crawl_url(doc):
                success += 1
        
        print(f"\nâœ… å®Œæˆ: {success}/{len(CORE_DOCS)}")
        if self.failed_urls:
            print(f"âŒ å¤±è´¥: {len(self.failed_urls)}")
    
    async def crawl_full(self):
        """æŠ“å–å…¨éƒ¨æ–‡æ¡£"""
        await self._init_browser()
        
        all_docs = CORE_DOCS + EXTENDED_DOCS
        success = 0
        for doc in all_docs:
            if await self.crawl_url(doc):
                success += 1
        
        print(f"\nâœ… å®Œæˆ: {success}/{len(all_docs)}")
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()


async def main_async(args):
    crawler = CookieCrawler(args.cookies, Path(args.output) if args.output else OUTPUT_DIR)
    try:
        if args.full:
            await crawler.crawl_full()
        else:
            await crawler.crawl_core()
    finally:
        await crawler.close()


def main():
    parser = argparse.ArgumentParser(description="Epic UEFN æ–‡æ¡£çˆ¬è™« (Cookie è¾…åŠ©)")
    parser.add_argument('--cookies', type=str, default='cookies.json', help='Cookies JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--full', action='store_true', help='æŠ“å–å…¨éƒ¨æ–‡æ¡£')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
