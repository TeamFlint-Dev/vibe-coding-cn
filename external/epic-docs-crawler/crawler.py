#!/usr/bin/env python3
"""
Epic Games UEFN Documentation Crawler
ä½¿ç”¨ Playwright ç»•è¿‡ Cloudflare ä¿æŠ¤æŠ“å–å®˜æ–¹æ–‡æ¡£

Usage:
    python crawler.py                    # æŠ“å–æ ¸å¿ƒ API æ–‡æ¡£
    python crawler.py --full             # æŠ“å–å…¨éƒ¨æ–‡æ¡£
    python crawler.py --url <url>        # æŠ“å–æŒ‡å®šé¡µé¢
"""

import os
import sys
import json
import time
import hashlib
import argparse
import asyncio
from pathlib import Path
from datetime import datetime
from urllib.parse import urljoin, urlparse

try:
    from playwright.async_api import async_playwright
    from playwright_stealth import Stealth
    from markdownify import markdownify as md
except ImportError:
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install playwright playwright-stealth markdownify")
    print("ç„¶åè¿è¡Œ: playwright install chromium --with-deps")
    sys.exit(1)


# ============== é…ç½® ==============

BASE_URL = "https://dev.epicgames.com/documentation/en-us/uefn"

# æ ¸å¿ƒæ–‡æ¡£ URLsï¼ˆä¼˜å…ˆæŠ“å–ï¼‰
CORE_DOCS = [
    "/verse-language-reference",
    "/verse-api",
    "/verse-language-quick-reference",
    "/specifiers-and-attributes-in-verse",
    "/modules-and-paths-in-verse",
    "/option-values-in-verse",
    "/failure-and-failable-expressions-in-verse",
    "/concurrency-in-verse",
    "/concurrency-overview-in-verse",
    "/classes-and-objects-in-verse",
    "/interfaces-in-verse",
    "/structs-in-verse",
    "/array-in-verse",
    "/map-in-verse",
    "/types-in-verse",
]

# æ‰©å±•æ–‡æ¡£ï¼ˆ--full æ¨¡å¼ï¼‰
EXTENDED_DOCS = [
    # Devices
    "/devices",
    "/using-devices-in-verse",
    "/creative-devices-in-verse",
    "/trigger-device-in-verse",
    "/button-device-in-verse",
    "/item-spawner-device-in-verse",
    "/timer-device-in-verse",
    "/hud-message-device-in-verse",
    "/scoreboard-device-in-verse",
    "/player-spawner-device-in-verse",
    "/teleporter-device-in-verse",
    "/elimination-manager-device-in-verse",
    # UI
    "/custom-ui-in-verse",
    "/creating-custom-ui",
    "/ui-widgets-in-verse",
    # Characters & Players
    "/characters-in-verse",
    "/players-in-verse",
    "/teams-in-verse",
    "/playspaces-in-verse",
    # Gameplay
    "/game-flow-in-verse",
    "/game-modes-in-verse",
    "/scoring-in-verse",
    "/rounds-in-verse",
    # Props & Assets
    "/props-in-verse",
    "/spawning-props-in-verse",
    "/assets-in-verse",
    # Animation & Effects
    "/animation-in-verse",
    "/visual-effects-in-verse",
    "/audio-in-verse",
    # Math & Spatial
    "/math-in-verse",
    "/transforms-in-verse",
    "/vectors-in-verse",
    "/rotations-in-verse",
    # Best Practices
    "/verse-style-guide",
    "/debugging-verse-code",
    "/verse-best-practices",
    "/performance-in-verse",
]

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(__file__).parent.parent.parent.parent / "i18n/zh/skills/uefn-dev/references/official-docs"


# ============== çˆ¬è™«å®ç° ==============

class EpicDocsCrawler:
    """Epic Games æ–‡æ¡£çˆ¬è™« (Playwright ç‰ˆ)"""
    
    def __init__(self, output_dir: Path = OUTPUT_DIR):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.browser = None
        self.context = None
        self.page = None
        self.playwright = None
        self.crawled_urls = set()
        self.failed_urls = []
        self.index = {}
        
        # åŠ è½½å·²çˆ¬å–çš„ç´¢å¼•
        self.index_file = self.output_dir / "index.json"
        if self.index_file.exists():
            with open(self.index_file, "r", encoding="utf-8") as f:
                self.index = json.load(f)
    
    async def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        
        # ä½¿ç”¨ Chromiumï¼Œæ·»åŠ åæ£€æµ‹å‚æ•°
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-infobars',
                '--window-size=1920,1080',
                '--start-maximized',
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
            timezone_id='America/New_York',
        )
        
        # æ·»åŠ åæ£€æµ‹è„šæœ¬
        await self.context.add_init_script("""
            // éšè— webdriver å±æ€§
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            // ä¼ªé€  plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            // ä¼ªé€  languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en']
            });
            
            // éšè— automation ç›¸å…³å±æ€§
            window.chrome = { runtime: {} };
        """)
        
        self.page = await self.context.new_page()
        
        # åº”ç”¨ stealth æ¨¡å¼
        stealth = Stealth()
        await stealth.apply_stealth_async(self.page)
        
        print("ğŸŒ æµè§ˆå™¨å·²å¯åŠ¨ (Playwright Chromium + Stealth)")
    
    async def _wait_for_cloudflare(self, timeout: int = 60):
        """ç­‰å¾… Cloudflare æŒ‘æˆ˜å®Œæˆ"""
        start = time.time()
        check_interval = 2
        
        while time.time() - start < timeout:
            try:
                title = await self.page.title()
                title_lower = title.lower() if title else ""
                
                # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨ Cloudflare é¡µé¢
                if "just a moment" in title_lower or "checking" in title_lower:
                    print(f"  â³ Cloudflare éªŒè¯ä¸­... ({int(time.time() - start)}s)")
                    await asyncio.sleep(check_interval)
                    continue
                
                # æ£€æŸ¥é¡µé¢å†…å®¹
                body = await self.page.content()
                if "cf-browser-verification" in body or "challenge-running" in body:
                    print(f"  â³ ç­‰å¾…éªŒè¯å®Œæˆ... ({int(time.time() - start)}s)")
                    await asyncio.sleep(check_interval)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
                content = await self.page.query_selector('article, main, .documentation-content')
                if content:
                    print("  âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                    return True
                
                # å¯èƒ½è¿˜åœ¨åŠ è½½
                await asyncio.sleep(check_interval)
                
            except Exception as e:
                print(f"  âš ï¸ æ£€æŸ¥çŠ¶æ€å‡ºé”™: {e}")
                await asyncio.sleep(check_interval)
        
        return False
    
    async def _extract_content(self) -> dict:
        """ä»å½“å‰é¡µé¢æå–æ–‡æ¡£å†…å®¹"""
        result = {
            "title": "",
            "content": "",
            "html": "",
            "breadcrumb": [],
            "links": [],
        }
        
        try:
            # ç­‰å¾…ä¸»å†…å®¹åŠ è½½
            await self.page.wait_for_selector('article, main, .content', timeout=15000)
        except:
            pass
        
        # æå–æ ‡é¢˜
        try:
            h1 = await self.page.query_selector('h1')
            if h1:
                result["title"] = await h1.inner_text()
        except:
            result["title"] = await self.page.title() or "Untitled"
        
        # æå–é¢åŒ…å±‘å¯¼èˆª
        try:
            breadcrumb = await self.page.query_selector('.breadcrumb, nav[aria-label="breadcrumb"]')
            if breadcrumb:
                links = await breadcrumb.query_selector_all('a')
                result["breadcrumb"] = [await a.inner_text() for a in links]
        except:
            pass
        
        # æå–ä¸»å†…å®¹
        content_selectors = [
            'article',
            'main article',
            '.documentation-content',
            '.content-body',
            '#content',
            '.markdown-body',
            'main',
        ]
        
        for selector in content_selectors:
            try:
                content_el = await self.page.query_selector(selector)
                if content_el:
                    result["html"] = await content_el.inner_html()
                    result["content"] = md(
                        result["html"],
                        heading_style="atx",
                        code_language="verse",
                        strip=['script', 'style', 'nav', 'footer', 'aside'],
                    )
                    if len(result["content"]) > 200:
                        break
            except:
                continue
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸»å†…å®¹ï¼Œä½¿ç”¨æ•´ä¸ªé¡µé¢
        if not result["content"] or len(result["content"]) < 200:
            try:
                full_html = await self.page.content()
                result["html"] = full_html
                result["content"] = md(
                    full_html,
                    heading_style="atx",
                    code_language="verse",
                    strip=['script', 'style', 'nav', 'footer', 'header', 'aside'],
                )
            except:
                result["content"] = "Failed to extract content"
        
        # æå–é¡µå†…é“¾æ¥ï¼ˆç”¨äºå‘ç°æ›´å¤šæ–‡æ¡£ï¼‰
        try:
            all_links = await self.page.query_selector_all('a[href*="/documentation/en-us/uefn/"]')
            for a in all_links:
                href = await a.get_attribute('href')
                if href:
                    result["links"].append(href)
        except:
            pass
        
        return result
    
    def _url_to_filename(self, url: str) -> str:
        """å°† URL è½¬æ¢ä¸ºæ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path.replace('/documentation/en-us/uefn/', '')
        path = path.strip('/')
        
        if not path:
            return "index"
        
        # æ¸…ç†æ–‡ä»¶å
        filename = path.replace('/', '_').replace('-', '_')
        return filename
    
    def _save_content(self, url: str, content: dict):
        """ä¿å­˜æŠ“å–çš„å†…å®¹"""
        filename = self._url_to_filename(url)
        filepath = self.output_dir / f"{filename}.md"
        
        # æ„å»º Markdown æ–‡æ¡£
        markdown = f"""---
title: "{content['title']}"
source: "{url}"
crawled_at: "{datetime.now().isoformat()}"
---

# {content['title']}

{content['content']}
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        # æ›´æ–°ç´¢å¼•
        self.index[url] = {
            "title": content["title"],
            "file": filepath.name,
            "crawled_at": datetime.now().isoformat(),
            "hash": hashlib.md5(content["content"].encode()).hexdigest(),
            "size": len(content["content"]),
        }
        
        # ä¿å­˜ç´¢å¼•
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, indent=2, ensure_ascii=False)
        
        print(f"  ğŸ’¾ å·²ä¿å­˜: {filepath.name} ({len(content['content'])/1024:.1f} KB)")
        return filepath
    
    async def crawl_url(self, url: str) -> bool:
        """æŠ“å–å•ä¸ª URL"""
        if url in self.crawled_urls:
            print(f"  â­ï¸  å·²æŠ“å–è¿‡: {url}")
            return True
        
        # æ­£ç¡®æ‹¼æ¥ URL
        if url.startswith('http'):
            full_url = url
        elif url.startswith('/'):
            full_url = BASE_URL + url
        else:
            full_url = BASE_URL + '/' + url
        print(f"\nğŸ“„ æ­£åœ¨æŠ“å–: {full_url}")
        
        try:
            # è®¿é—®é¡µé¢
            response = await self.page.goto(full_url, wait_until='domcontentloaded', timeout=60000)
            
            if response and response.status == 403:
                print(f"  âŒ 403 Forbidden - Cloudflare æ‹¦æˆª")
                self.failed_urls.append(full_url)
                return False
            
            # ç­‰å¾… Cloudflare éªŒè¯
            if not await self._wait_for_cloudflare():
                print(f"  âŒ Cloudflare éªŒè¯è¶…æ—¶æˆ–å¤±è´¥")
                self.failed_urls.append(full_url)
                return False
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹åŠ è½½
            await asyncio.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦ 404
            title = await self.page.title()
            if "404" in (title or ""):
                print(f"  âŒ 404 Not Found")
                self.failed_urls.append(full_url)
                return False
            
            # æå–å†…å®¹
            content = await self._extract_content()
            
            if not content["content"] or len(content["content"]) < 100:
                print(f"  âš ï¸  å†…å®¹è¿‡å°‘ ({len(content.get('content', ''))} chars)ï¼Œé‡è¯•ä¸­...")
                await asyncio.sleep(5)
                content = await self._extract_content()
            
            if len(content["content"]) < 100:
                print(f"  âŒ æ— æ³•è·å–æœ‰æ•ˆå†…å®¹")
                self.failed_urls.append(full_url)
                return False
            
            # ä¿å­˜
            self._save_content(full_url, content)
            self.crawled_urls.add(full_url)
            
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
            delay = 5 + (hash(url) % 5)
            print(f"  â³ ç­‰å¾… {delay}s...")
            await asyncio.sleep(delay)
            
            return True
            
        except Exception as e:
            print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
            self.failed_urls.append(full_url)
            return False
    
    async def crawl_core(self):
        """æŠ“å–æ ¸å¿ƒæ–‡æ¡£"""
        print("\n" + "="*60)
        print("ğŸ¯ å¼€å§‹æŠ“å–æ ¸å¿ƒ UEFN/Verse æ–‡æ¡£")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*60)
        
        await self._init_browser()
        
        # å…ˆè®¿é—®é¦–é¡µå»ºç«‹ session
        print("\nğŸ“¡ æ­£åœ¨å»ºç«‹è¿æ¥...")
        await self.page.goto(BASE_URL, wait_until='domcontentloaded', timeout=60000)
        
        if not await self._wait_for_cloudflare(timeout=90):
            print("âŒ æ— æ³•é€šè¿‡ Cloudflare éªŒè¯")
            await self._print_summary(0, len(CORE_DOCS))
            return
        
        print("âœ… Cloudflare éªŒè¯é€šè¿‡ï¼å¼€å§‹æŠ“å–æ–‡æ¡£...")
        await asyncio.sleep(3)
        
        # æŠ“å–æ ¸å¿ƒæ–‡æ¡£
        success = 0
        for doc_path in CORE_DOCS:
            if await self.crawl_url(doc_path):
                success += 1
        
        await self._print_summary(success, len(CORE_DOCS))
    
    async def crawl_full(self):
        """æŠ“å–å…¨éƒ¨æ–‡æ¡£"""
        print("\n" + "="*60)
        print("ğŸ¯ å¼€å§‹æŠ“å–å…¨éƒ¨ UEFN/Verse æ–‡æ¡£")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*60)
        
        await self._init_browser()
        
        # å…ˆè®¿é—®é¦–é¡µ
        print("\nğŸ“¡ æ­£åœ¨å»ºç«‹è¿æ¥...")
        await self.page.goto(BASE_URL, wait_until='domcontentloaded', timeout=60000)
        
        if not await self._wait_for_cloudflare(timeout=90):
            print("âŒ æ— æ³•é€šè¿‡ Cloudflare éªŒè¯")
            return
        
        print("âœ… Cloudflare éªŒè¯é€šè¿‡ï¼å¼€å§‹æŠ“å–æ–‡æ¡£...")
        await asyncio.sleep(3)
        
        # åˆå¹¶æ‰€æœ‰ URLs
        all_docs = CORE_DOCS + EXTENDED_DOCS
        
        success = 0
        for doc_path in all_docs:
            if await self.crawl_url(doc_path):
                success += 1
        
        await self._print_summary(success, len(all_docs))
    
    async def crawl_single(self, url: str):
        """æŠ“å–å•ä¸ªé¡µé¢"""
        await self._init_browser()
        
        # å…ˆè®¿é—®é¦–é¡µå»ºç«‹ session
        print("\nğŸ“¡ æ­£åœ¨å»ºç«‹è¿æ¥...")
        await self.page.goto(BASE_URL, wait_until='domcontentloaded', timeout=60000)
        await self._wait_for_cloudflare(timeout=90)
        await asyncio.sleep(3)
        
        success = await self.crawl_url(url)
        
        await self._print_summary(1 if success else 0, 1)
    
    async def _print_summary(self, success: int, total: int):
        """æ‰“å°æŠ“å–æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æŠ“å–å®Œæˆ")
        print("="*60)
        print(f"  âœ… æˆåŠŸ: {success}/{total}")
        print(f"  âŒ å¤±è´¥: {len(self.failed_urls)}")
        print(f"  ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        if self.failed_urls:
            print("\nâŒ å¤±è´¥çš„ URLs:")
            for url in self.failed_urls[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                print(f"    - {url}")
            if len(self.failed_urls) > 10:
                print(f"    ... è¿˜æœ‰ {len(self.failed_urls) - 10} ä¸ª")
        
        # åˆ—å‡ºä¿å­˜çš„æ–‡ä»¶
        saved_files = list(self.output_dir.glob("*.md"))
        if saved_files:
            print(f"\nğŸ“ å·²ä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶:")
            total_size = 0
            for f in sorted(saved_files)[:15]:
                size = f.stat().st_size / 1024
                total_size += size
                print(f"    {f.name} ({size:.1f} KB)")
            if len(saved_files) > 15:
                print(f"    ... è¿˜æœ‰ {len(saved_files) - 15} ä¸ªæ–‡ä»¶")
            print(f"\n  ğŸ“¦ æ€»å¤§å°: {total_size:.1f} KB")
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()


async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    output_dir = Path(args.output) if args.output else OUTPUT_DIR
    crawler = EpicDocsCrawler(output_dir=output_dir)
    
    try:
        if args.url:
            await crawler.crawl_single(args.url)
        elif args.full:
            await crawler.crawl_full()
        else:
            await crawler.crawl_core()
    finally:
        await crawler.close()


def main():
    parser = argparse.ArgumentParser(description="Epic Games UEFN æ–‡æ¡£çˆ¬è™« (Playwright)")
    parser.add_argument('--full', action='store_true', help='æŠ“å–å…¨éƒ¨æ–‡æ¡£')
    parser.add_argument('--url', type=str, help='æŠ“å–æŒ‡å®š URL')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
