#!/usr/bin/env python3
"""
GitHub Webhook ä¸­è½¬ç«™ (Central Hub)

åŠŸèƒ½ï¼š
1. æ¥æ”¶ GitHub äº‹ä»¶ï¼ˆPRã€è¯„è®ºï¼‰ï¼Œè§¦å‘ GitHub Actions
2. æ¥æ”¶ Actions å›è°ƒï¼Œæ ¹æ®ç»“æœå†³å®šä¸‹ä¸€æ­¥æ“ä½œ
3. å¤šè´¦å·ç®¡ç†ï¼Œè‡ªåŠ¨åˆ‡æ¢ä»¥ @copilot è¯·æ±‚ä¿®å¤
4. çŠ¶æ€è·Ÿè¸ªï¼Œæ”¯æŒæŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

æ¶æ„ï¼š
  GitHub Events â†’ /webhook â†’ Decision â†’ repository_dispatch â†’ Actions
                                â†‘
  Actions Result â†’ /callback â”€â”€â”€â”˜
"""

import hashlib
import hmac
import json
import os
import re
import sys
from http.server import HTTPServer, BaseHTTPRequestHandler
from typing import Optional

# æœ¬åœ°æ¨¡å—
from state_store import get_store, TaskStatus
from account_manager import get_account_manager
from github_client import get_github_client
from decision_engine import get_decision_engine
from pipeline_scheduler import get_scheduler, init_scheduler

# ==================== é…ç½® ====================
WEBHOOK_SECRET = os.environ.get("WEBHOOK_SECRET", "")
CALLBACK_SECRET = os.environ.get("CALLBACK_SECRET", "")
PIPELINE_SECRET = os.environ.get("PIPELINE_SECRET", "")  # Pipeline ä¸“ç”¨å¯†é’¥
PORT = int(os.environ.get("PORT", "8080"))

# Pipeline é…ç½®
PIPELINE_REPO_PATH = os.environ.get("PIPELINE_REPO_PATH", "/opt/pipeline-repo")

# æ‰‹åŠ¨æ„å»ºå‘½ä»¤æ¨¡å¼
BUILD_COMMANDS = re.compile(r"^\s*(/build|/ç¼–è¯‘)\s*$", re.MULTILINE)


# ==================== æ—¥å¿— ====================
def log(message: str):
    """ç®€å•æ—¥å¿—"""
    print(f"[hub] {message}", flush=True)


# ==================== ç­¾åéªŒè¯ ====================
def verify_github_signature(payload: bytes, signature: str) -> bool:
    """éªŒè¯ GitHub webhook ç­¾å"""
    if not WEBHOOK_SECRET:
        log("WARNING: WEBHOOK_SECRET not set, skipping verification")
        return True
    
    if not signature or not signature.startswith("sha256="):
        log("ERROR: Invalid signature format")
        return False
    
    expected = "sha256=" + hmac.new(
        WEBHOOK_SECRET.encode(),
        payload,
        hashlib.sha256
    ).hexdigest()
    
    return hmac.compare_digest(expected, signature)


def verify_callback_signature(payload: bytes, signature: str) -> bool:
    """éªŒè¯ Actions å›è°ƒç­¾å"""
    if not CALLBACK_SECRET:
        log("WARNING: CALLBACK_SECRET not set, skipping verification")
        return True
    
    if not signature or not signature.startswith("sha256="):
        log("ERROR: Invalid callback signature format")
        return False
    
    expected = "sha256=" + hmac.new(
        CALLBACK_SECRET.encode(),
        payload,
        hashlib.sha256
    ).hexdigest()
    
    return hmac.compare_digest(expected, signature)


def verify_pipeline_signature(payload: bytes, signature: str) -> bool:
    """éªŒè¯ Pipeline è¯·æ±‚ç­¾å"""
    if not PIPELINE_SECRET:
        log("WARNING: PIPELINE_SECRET not set, skipping verification")
        return True
    
    if not signature or not signature.startswith("sha256="):
        log("ERROR: Invalid pipeline signature format")
        return False
    
    expected = "sha256=" + hmac.new(
        PIPELINE_SECRET.encode(),
        payload,
        hashlib.sha256
    ).hexdigest()
    
    return hmac.compare_digest(expected, signature)


# ==================== HTTP Handler ====================
class HubHandler(BaseHTTPRequestHandler):
    """ä¸­è½¬ç«™ HTTP å¤„ç†å™¨"""
    
    def __init__(self, *args, **kwargs):
        self.store = get_store()
        self.github = get_github_client()
        self.engine = get_decision_engine()
        self.accounts = get_account_manager()
        super().__init__(*args, **kwargs)
    
    def log_message(self, format, *args):
        """é‡å†™æ—¥å¿—æ ¼å¼"""
        log(f"HTTP {args[0]}")
    
    def _send_json(self, status: int, data: dict):
        """å‘é€ JSON å“åº”"""
        self.send_response(status)
        self.send_header("Content-Type", "application/json")
        self.end_headers()
        self.wfile.write(json.dumps(data).encode())
    
    # ==================== GET ç«¯ç‚¹ ====================
    
    def do_GET(self):
        """å¤„ç† GET è¯·æ±‚"""
        if self.path == "/health":
            self._handle_health()
        elif self.path.startswith("/status/"):
            task_id = self.path[8:]  # å»æ‰ "/status/"
            self._handle_status(task_id)
        elif self.path == "/accounts":
            self._handle_accounts()
        elif self.path == "/stats":
            self._handle_stats()
        # Pipeline ç›¸å…³ç«¯ç‚¹
        elif self.path.startswith("/pipeline/status/"):
            pipeline_id = self.path[17:]  # å»æ‰ "/pipeline/status/"
            self._handle_pipeline_status(pipeline_id)
        elif self.path == "/pipeline/list":
            self._handle_pipeline_list()
        else:
            self._send_json(404, {"error": "Not found"})
    
    def _handle_health(self):
        """å¥åº·æ£€æŸ¥"""
        self._send_json(200, {
            "status": "ok",
            "accounts_available": self.accounts.has_available_accounts()
        })
    
    def _handle_status(self, task_id: str):
        """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
        task = self.store.get_task(task_id)
        if task:
            self._send_json(200, task.to_dict())
        else:
            self._send_json(404, {"error": "Task not found"})
    
    def _handle_accounts(self):
        """æŸ¥è¯¢è´¦å·çŠ¶æ€"""
        self._send_json(200, self.accounts.get_stats())
    
    def _handle_stats(self):
        """æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""
        self._send_json(200, {
            "tasks": self.store.get_stats(),
            "accounts": self.accounts.get_stats()
        })
    
    # ==================== Pipeline GET ç«¯ç‚¹ ====================
    
    def _handle_pipeline_status(self, pipeline_id: str):
        """æŸ¥è¯¢æµæ°´çº¿çŠ¶æ€"""
        scheduler = get_scheduler()
        if not scheduler:
            self._send_json(503, {"error": "Pipeline scheduler not initialized"})
            return
        
        pipeline = scheduler.get_pipeline(pipeline_id)
        if pipeline:
            self._send_json(200, pipeline.to_dict())
        else:
            self._send_json(404, {"error": "Pipeline not found"})
    
    def _handle_pipeline_list(self):
        """åˆ—å‡ºæ‰€æœ‰æµæ°´çº¿"""
        scheduler = get_scheduler()
        if not scheduler:
            self._send_json(503, {"error": "Pipeline scheduler not initialized"})
            return
        
        pipelines = scheduler.get_all_pipelines()
        self._send_json(200, {
            "count": len(pipelines),
            "pipelines": [p.to_dict() for p in pipelines]
        })
    
    # ==================== POST ç«¯ç‚¹ ====================
    
    def do_POST(self):
        """å¤„ç† POST è¯·æ±‚"""
        content_length = int(self.headers.get("Content-Length", 0))
        payload = self.rfile.read(content_length)
        
        if self.path == "/webhook":
            self._handle_webhook(payload)
        elif self.path == "/callback":
            self._handle_callback(payload)
        # Pipeline ç›¸å…³ç«¯ç‚¹
        elif self.path == "/pipeline/ready":
            self._handle_pipeline_ready(payload)
        elif self.path.startswith("/pipeline/cancel/"):
            pipeline_id = self.path[17:]  # å»æ‰ "/pipeline/cancel/"
            self._handle_pipeline_cancel(pipeline_id)
        else:
            self._send_json(404, {"error": "Not found"})
    
    # ==================== Pipeline POST ç«¯ç‚¹ ====================
    
    def _handle_pipeline_ready(self, payload: bytes):
        """å¤„ç† Planner Agent çš„æµæ°´çº¿å¯åŠ¨é€šçŸ¥
        
        è¯·æ±‚æ ¼å¼:
        {
            "pipeline_id": "p20260101120000",
            "type": "skills-distill",
            "stages": ["ingest", "classify", "extract", "assemble", "validate"],
            "stage_ids": {"ingest": "bd-abc123", "classify": "bd-def456", ...},  # å¯é€‰
            "source_url": "https://..."  # å¯é€‰
        }
        """
        import asyncio
        
        # éªŒè¯ç­¾å
        signature = self.headers.get("X-Pipeline-Signature", "")
        if not verify_pipeline_signature(payload, signature):
            log("Pipeline ready: Invalid signature")
            self._send_json(401, {"error": "Invalid signature"})
            return
        
        try:
            data = json.loads(payload)
        except json.JSONDecodeError as e:
            log(f"Pipeline ready: Invalid JSON - {e}")
            self._send_json(400, {"error": f"Invalid JSON: {e}"})
            return
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        pipeline_id = data.get("pipeline_id")
        pipeline_type = data.get("type")
        stages = data.get("stages", [])
        
        if not pipeline_id:
            self._send_json(400, {"error": "Missing required field: pipeline_id"})
            return
        if not pipeline_type:
            self._send_json(400, {"error": "Missing required field: type"})
            return
        if not stages:
            self._send_json(400, {"error": "Missing required field: stages (must be non-empty array)"})
            return
        
        # å¯é€‰å­—æ®µ
        stage_ids = data.get("stage_ids", {})
        source_url = data.get("source_url", "")
        branch = data.get("branch", "")  # å·¥ä½œåˆ†æ”¯åç§°
        
        log(f"Pipeline ready: id={pipeline_id}, type={pipeline_type}, stages={stages}")
        if stage_ids:
            log(f"  stage_ids: {stage_ids}")
        if branch:
            log(f"  branch: {branch}")
        
        scheduler = get_scheduler()
        if not scheduler:
            log("Pipeline ready: Scheduler not initialized")
            self._send_json(503, {"error": "Pipeline scheduler not initialized"})
            return
        
        # å¯åŠ¨æµæ°´çº¿
        try:
            # åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            pipeline = loop.run_until_complete(
                scheduler.start_pipeline(
                    pipeline_id=pipeline_id,
                    pipeline_type=pipeline_type,
                    stages=stages,
                    stage_ids=stage_ids,
                    source_url=source_url,
                    branch=branch if branch else None
                )
            )
            
            log(f"Pipeline {pipeline_id} started, issue #{pipeline.issue_number}")
            self._send_json(200, {
                "status": "accepted",
                "pipeline_id": pipeline_id,
                "type": pipeline_type,
                "stages": stages,
                "issue_number": pipeline.issue_number,
                "message": f"Pipeline started successfully. Track at issue #{pipeline.issue_number}"
            })
        except Exception as e:
            log(f"Failed to start pipeline {pipeline_id}: {e}")
            import traceback
            traceback.print_exc()
            self._send_json(500, {"error": str(e), "pipeline_id": pipeline_id})
    
    def _handle_pipeline_cancel(self, pipeline_id: str):
        """å–æ¶ˆæµæ°´çº¿"""
        import asyncio
        
        scheduler = get_scheduler()
        if not scheduler:
            self._send_json(503, {"error": "Pipeline scheduler not initialized"})
            return
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            success = loop.run_until_complete(scheduler.cancel_pipeline(pipeline_id))
            
            if success:
                self._send_json(200, {"status": "cancelled", "pipeline_id": pipeline_id})
            else:
                self._send_json(404, {"error": "Pipeline not found"})
        except Exception as e:
            log(f"Failed to cancel pipeline: {e}")
            self._send_json(500, {"error": str(e)})
    
    # ==================== Webhook å¤„ç† ====================

    def _handle_webhook(self, payload: bytes):
        """å¤„ç† GitHub Webhook"""
        # éªŒè¯ç­¾å
        signature = self.headers.get("X-Hub-Signature-256", "")
        if not verify_github_signature(payload, signature):
            self._send_json(401, {"error": "Invalid signature"})
            return
        
        # è§£æäº‹ä»¶
        event_type = self.headers.get("X-GitHub-Event", "")
        log(f"Received GitHub event: {event_type}")
        
        try:
            data = json.loads(payload)
        except json.JSONDecodeError as e:
            self._send_json(400, {"error": f"Invalid JSON: {e}"})
            return
        
        # è·¯ç”±åˆ°å…·ä½“å¤„ç†å™¨
        if event_type == "pull_request":
            self._handle_pr_event(data)
        elif event_type == "issue_comment":
            self._handle_comment_event(data)
        elif event_type == "issues":
            self._handle_issues_event(data)
        elif event_type == "workflow_run":
            self._handle_workflow_run_event(data)
        elif event_type == "ping":
            self._send_json(200, {"message": "pong"})
        else:
            log(f"Ignoring event: {event_type}")
            self._send_json(200, {"message": "Event ignored"})
    
    def _handle_workflow_run_event(self, data: dict):
        """å¤„ç† workflow_run äº‹ä»¶ - å½“ Planner Agent å®Œæˆæ—¶å¯åŠ¨æµæ°´çº¿"""
        import asyncio
        import urllib.request
        import zipfile
        import io
        
        action = data.get("action", "")
        workflow_run = data.get("workflow_run", {})
        workflow_name = workflow_run.get("name", "")
        conclusion = workflow_run.get("conclusion", "")
        run_id = workflow_run.get("id", 0)
        
        log(f"Workflow run: name={workflow_name}, action={action}, conclusion={conclusion}")
        
        # åªå¤„ç† planner-agent workflow çš„æˆåŠŸå®Œæˆ
        if action != "completed":
            self._send_json(200, {"message": f"Ignoring workflow action: {action}"})
            return
            
        if workflow_name != "planner-agent":
            self._send_json(200, {"message": f"Ignoring workflow: {workflow_name}"})
            return
            
        if conclusion != "success":
            log(f"Planner workflow failed with conclusion: {conclusion}")
            self._send_json(200, {"message": f"Planner failed: {conclusion}"})
            return
        
        log(f"Planner Agent completed successfully, fetching artifacts from run {run_id}")
        
        # ä» workflow artifacts è·å– pipeline ä¿¡æ¯
        try:
            pipeline_info = self._fetch_pipeline_artifact(run_id)
            if not pipeline_info:
                log("No pipeline artifact found")
                self._send_json(200, {"message": "No pipeline artifact found"})
                return
            
            log(f"Pipeline info from artifact: {pipeline_info}")
            
            # å¯åŠ¨è°ƒåº¦å™¨
            scheduler = get_scheduler()
            if not scheduler:
                log("Pipeline scheduler not initialized")
                self._send_json(503, {"error": "Pipeline scheduler not initialized"})
                return
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            pipeline = loop.run_until_complete(
                scheduler.start_pipeline(
                    pipeline_id=pipeline_info["pipeline_id"],
                    pipeline_type=pipeline_info.get("type", "unknown"),
                    stages=pipeline_info.get("stages", []),
                    stage_ids=pipeline_info.get("stage_ids", {}),
                    source_url=pipeline_info.get("source_url", "")
                )
            )
            log(f"Pipeline {pipeline_info['pipeline_id']} started from workflow_run")
            self._send_json(200, {
                "message": "Pipeline started",
                "pipeline_id": pipeline_info["pipeline_id"]
            })
        except Exception as e:
            log(f"Failed to start pipeline from workflow_run: {e}")
            self._send_json(500, {"error": str(e)})
    
    def _fetch_pipeline_artifact(self, run_id: int) -> Optional[dict]:
        """ä» workflow run çš„ artifacts è·å– pipeline ä¿¡æ¯"""
        import urllib.request
        import zipfile
        import io
        
        # GitHub API: è·å– artifacts åˆ—è¡¨
        token = os.environ.get("GITHUB_TOKEN", "")
        if not token:
            log("GITHUB_TOKEN not set, cannot fetch artifacts")
            return None
        
        repo = os.environ.get("GITHUB_REPO", "TeamFlint-Dev/vibe-coding-cn")
        url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/artifacts"
        
        req = urllib.request.Request(url)
        req.add_header("Authorization", f"Bearer {token}")
        req.add_header("Accept", "application/vnd.github+json")
        req.add_header("X-GitHub-Api-Version", "2022-11-28")
        
        try:
            with urllib.request.urlopen(req, timeout=30) as response:
                data = json.loads(response.read().decode())
        except Exception as e:
            log(f"Failed to fetch artifacts list: {e}")
            return None
        
        # æŸ¥æ‰¾ pipeline-info artifact
        artifacts = data.get("artifacts", [])
        pipeline_artifact = None
        for artifact in artifacts:
            if artifact.get("name") == "pipeline-info":
                pipeline_artifact = artifact
                break
        
        if not pipeline_artifact:
            log("No pipeline-info artifact found")
            return None
        
        # ä¸‹è½½ artifact (æ˜¯ä¸€ä¸ª zip æ–‡ä»¶)
        download_url = pipeline_artifact.get("archive_download_url")
        if not download_url:
            log("No download URL for artifact")
            return None
        
        req = urllib.request.Request(download_url)
        req.add_header("Authorization", f"Bearer {token}")
        req.add_header("Accept", "application/vnd.github+json")
        
        try:
            with urllib.request.urlopen(req, timeout=60) as response:
                zip_data = response.read()
        except Exception as e:
            log(f"Failed to download artifact: {e}")
            return None
        
        # è§£å‹å¹¶è¯»å– pipeline.json
        try:
            with zipfile.ZipFile(io.BytesIO(zip_data)) as zf:
                with zf.open("pipeline.json") as f:
                    return json.loads(f.read().decode())
        except Exception as e:
            log(f"Failed to parse artifact: {e}")
            return None

    def _handle_issues_event(self, data: dict):
        """å¤„ç† Issue äº‹ä»¶ - ç”¨äº Pipeline è‡ªåŠ¨è°ƒåº¦"""
        import asyncio
        
        action = data.get("action", "")
        issue = data.get("issue", {})
        issue_number = issue.get("number", 0)
        title = issue.get("title", "")
        body = issue.get("body", "")
        labels = [l.get("name", "") for l in issue.get("labels", [])]
        
        log(f"Issue #{issue_number}: action={action}, labels={labels}")
        
        # åªå¤„ç†å¸¦ pipeline æ ‡ç­¾çš„æ–° Issue
        if action != "opened" or "pipeline" not in labels:
            self._send_json(200, {"message": "Issue ignored (not a pipeline trigger)"})
            return
        
        # è§£æ Pipeline ä¿¡æ¯
        pipeline_info = self._parse_pipeline_from_issue(title, body)
        if not pipeline_info:
            log(f"Could not parse pipeline info from issue #{issue_number}")
            self._send_json(200, {"message": "Could not parse pipeline info"})
            return
        
        log(f"Parsed pipeline: {pipeline_info['pipeline_id']}, stages: {pipeline_info['stages']}")
        
        # å¯åŠ¨è°ƒåº¦å™¨
        scheduler = get_scheduler()
        if not scheduler:
            log("Pipeline scheduler not initialized")
            self._send_json(503, {"error": "Pipeline scheduler not initialized"})
            return
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            pipeline = loop.run_until_complete(
                scheduler.start_pipeline(
                    pipeline_id=pipeline_info["pipeline_id"],
                    pipeline_type=pipeline_info.get("type", "unknown"),
                    stages=pipeline_info["stages"],
                    stage_ids=pipeline_info.get("stage_ids", {}),
                    source_url=pipeline_info.get("source_url", ""),
                    issue_number=issue_number
                )
            )
            log(f"Pipeline {pipeline_info['pipeline_id']} started from issue #{issue_number}")
            self._send_json(200, {
                "message": "Pipeline started",
                "pipeline_id": pipeline_info["pipeline_id"],
                "issue_number": issue_number
            })
        except Exception as e:
            log(f"Failed to start pipeline: {e}")
            self._send_json(500, {"error": str(e)})
    
    def _parse_pipeline_from_issue(self, title: str, body: str) -> Optional[dict]:
        """ä» Issue å†…å®¹è§£æ Pipeline ä¿¡æ¯"""
        import re
        
        result = {}
        
        # ä»æ ‡é¢˜è§£æ pipeline_id (æ ¼å¼: "Pipeline xxx ...")
        # æ”¯æŒå¤šç§æ ¼å¼: p20260102150202, test004, my-pipeline-1 ç­‰
        title_match = re.search(r'Pipeline\s+([^\s-]+(?:-[^\s]+)?)', title, re.IGNORECASE)
        if title_match:
            result["pipeline_id"] = title_match.group(1)
        else:
            # å°è¯•ä» body è§£æ
            body_match = re.search(r'\*\*Pipeline ID\*\*:\s*`?([^`\s]+)`?', body)
            if body_match:
                result["pipeline_id"] = body_match.group(1)
            else:
                log(f"Could not find pipeline_id in title or body")
                return None
        
        # è§£æ type
        type_match = re.search(r'\*\*Type\*\*:\s*`?([a-z0-9-]+)`?', body, re.IGNORECASE)
        if type_match:
            result["type"] = type_match.group(1)
        
        # è§£æ source_url
        source_match = re.search(r'\*\*Source\*\*:\s*`?([^`\s]+)`?', body)
        if source_match:
            result["source_url"] = source_match.group(1)
        
        # è§£æ stages å’Œ stage_ids ä»è¡¨æ ¼
        # | Stage | Beads ID | ...
        # | 1. ingest | `xxx-yyy-zzz` | ...
        stages = []
        stage_ids = {}
        stage_pattern = re.compile(
            r'\|\s*\d+\.\s*(\w+)\s*\|\s*`([^`]+)`\s*\|',
            re.IGNORECASE
        )
        for match in stage_pattern.finditer(body):
            stage_name = match.group(1).lower()
            stage_id = match.group(2)
            stages.append(stage_name)
            stage_ids[stage_name] = stage_id
            log(f"Parsed stage: {stage_name} -> {stage_id}")
        
        if stages:
            result["stages"] = stages
            result["stage_ids"] = stage_ids
        else:
            # å°è¯•ä» JSON å—è§£æ
            json_match = re.search(r'"stages":\s*\[([^\]]+)\]', body)
            if json_match:
                try:
                    stages_str = json_match.group(1)
                    stages = [s.strip().strip('"').strip("'") for s in stages_str.split(",")]
                    result["stages"] = stages
                except:
                    pass
        
        if not result.get("stages"):
            return None
        
        return result
    
    def _handle_pr_event(self, data: dict):
        """å¤„ç† PR äº‹ä»¶"""
        action = data.get("action", "")
        pr = data.get("pull_request", {})
        pr_number = pr.get("number", 0)
        pr_title = pr.get("title", "")
        head_ref = pr.get("head", {}).get("ref", "")
        head_sha = pr.get("head", {}).get("sha", "")
        user_login = pr.get("user", {}).get("login", "")
        
        log(f"PR #{pr_number}: action={action}, user={user_login}")
        
        # åªå¤„ç† opened å’Œ synchronize
        if action not in ("opened", "synchronize"):
            self._send_json(200, {"message": f"Action '{action}' ignored"})
            return
        
        # åˆ›å»ºä»»åŠ¡
        task_id = self.store.create_task(
            pr_number=pr_number,
            head_sha=head_sha,
            head_ref=head_ref,
            trigger_type=f"pr-{action}"
        )
        
        log(f"Created task {task_id} for PR #{pr_number}")
        
        # è§¦å‘ repository_dispatch
        success = self.github.trigger_dispatch(
            event_type="build-pr",
            client_payload={
                "task_id": task_id,
                "pr_number": pr_number,
                "pr_title": pr_title,
                "head_ref": head_ref,
                "head_sha": head_sha
            }
        )
        
        if success:
            self.store.update_task(
                task_id,
                status=TaskStatus.BUILDING,
                event="dispatch_sent"
            )
            self._send_json(200, {
                "message": "Build dispatched",
                "task_id": task_id
            })
        else:
            self._send_json(500, {"error": "Failed to dispatch"})
    
    def _handle_comment_event(self, data: dict):
        """å¤„ç†è¯„è®ºäº‹ä»¶"""
        action = data.get("action", "")
        
        if action != "created":
            self._send_json(200, {"message": "Action ignored"})
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ PR è¯„è®º
        issue = data.get("issue", {})
        if "pull_request" not in issue:
            self._send_json(200, {"message": "Not a PR comment"})
            return
        
        comment = data.get("comment", {})
        comment_body = comment.get("body", "")
        comment_user = comment.get("user", {}).get("login", "")
        pr_number = issue.get("number", 0)
        
        # æ£€æµ‹ Copilot é¢åº¦è€—å°½æ¶ˆæ¯
        if self._check_copilot_quota_message(comment_body, comment_user, pr_number):
            self._send_json(200, {"message": "Copilot quota exhausted handled"})
            return
        
        # æ£€æµ‹æ„å»ºå‘½ä»¤
        if not BUILD_COMMANDS.search(comment_body):
            self._send_json(200, {"message": "No build command"})
            return
        
        log(f"Build command from {comment_user} on PR #{pr_number}")
        
        # è·å– PR ä¿¡æ¯
        pr_info = self.github.get_pr_info(pr_number)
        if not pr_info:
            self._send_json(500, {"error": "Failed to get PR info"})
            return
        
        head_ref = pr_info.get("head", {}).get("ref", "")
        head_sha = pr_info.get("head", {}).get("sha", "")
        pr_title = pr_info.get("title", "")
        
        # åˆ›å»ºä»»åŠ¡
        task_id = self.store.create_task(
            pr_number=pr_number,
            head_sha=head_sha,
            head_ref=head_ref,
            trigger_type=f"command-{comment_user}"
        )
        
        log(f"Created task {task_id} for manual build")
        
        # è§¦å‘æ„å»º
        success = self.github.trigger_dispatch(
            event_type="build-pr",
            client_payload={
                "task_id": task_id,
                "pr_number": pr_number,
                "pr_title": pr_title,
                "head_ref": head_ref,
                "head_sha": head_sha
            }
        )
        
        if success:
            self.store.update_task(
                task_id,
                status=TaskStatus.BUILDING,
                event="dispatch_sent",
                event_details=f"Manual trigger by {comment_user}"
            )
            self._send_json(200, {
                "message": "Build dispatched",
                "task_id": task_id,
                "triggered_by": comment_user
            })
        else:
            self._send_json(500, {"error": "Failed to dispatch"})
    
    def _check_copilot_quota_message(self, comment_body: str, comment_user: str, pr_number: int) -> bool:
        """
        æ£€æµ‹ Copilot é¢åº¦è€—å°½æ¶ˆæ¯ï¼Œè§¦å‘è´¦å·åˆ‡æ¢
        
        æ¶ˆæ¯ç¤ºä¾‹ï¼š
        "Copilot stopped work on behalf of Maybank01 due to an error...
        Your session could not start because you've used up the 300 premium requests allowance..."
        
        è¿”å›: True å¦‚æœæ£€æµ‹åˆ°å¹¶å¤„ç†äº†é¢åº¦è€—å°½æ¶ˆæ¯
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Copilot é¢åº¦è€—å°½æ¶ˆæ¯
        if not self.accounts.check_copilot_quota_exhausted(comment_body):
            return False
        
        # ä»æ¶ˆæ¯ä¸­æå–è¢«ç¦ç”¨çš„è´¦å·å
        # æ ¼å¼: "Copilot stopped work on behalf of USERNAME due to an error"
        import re
        match = re.search(r"copilot stopped work on behalf of (\w+)", comment_body.lower())
        
        if match:
            exhausted_username = match.group(1)
            log(f"Detected Copilot quota exhausted for user: {exhausted_username}")
            
            # ç¦ç”¨è¯¥è´¦å·
            disabled = self.accounts.disable_account_for_quota(exhausted_username)
            
            if disabled:
                log(f"Account {exhausted_username} disabled due to Copilot quota exhaustion")
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¯ç”¨è´¦å·
                if self.accounts.has_available_accounts():
                    # å‘é€é€šçŸ¥å¹¶ç”¨æ–°è´¦å·é‡æ–°è¯·æ±‚ Copilot
                    next_account = self.accounts.get_active_account()
                    if next_account:
                        log(f"Switching to account: {next_account.username}")
                        
                        # å‘é€ç³»ç»Ÿé€šçŸ¥
                        self.github.post_comment(
                            pr_number,
                            f"âš ï¸ **Copilot Quota Exhausted**\n\n"
                            f"Account `{exhausted_username}` has used up its Copilot premium requests quota.\n"
                            f"Switching to account `{next_account.username}` and retrying...\n\n"
                            f"<!-- quota-switch: {exhausted_username} -> {next_account.username} -->",
                            use_user_account=False  # ç”¨ bot å‘é€šçŸ¥
                        )
                        
                        # ç”¨æ–°è´¦å·é‡æ–° @copilot
                        self.github.post_comment(
                            pr_number,
                            "@copilot Please continue fixing the issues in this PR.",
                            use_user_account=True  # ç”¨æ–°è´¦å·å‘
                        )
                else:
                    # æ²¡æœ‰å¯ç”¨è´¦å·äº†ï¼Œé€šçŸ¥äººå·¥
                    log("No more available accounts, escalating to human")
                    notify_user = os.environ.get("NOTIFY_USER", "")
                    mention = f"@{notify_user}" if notify_user else "maintainer"
                    
                    self.github.post_comment(
                        pr_number,
                        f"ğŸš¨ **All Copilot Accounts Exhausted**\n\n"
                        f"All configured user accounts have exceeded their Copilot premium request quotas.\n\n"
                        f"{mention} Please investigate manually or wait for quota reset.\n\n"
                        f"<!-- all-accounts-exhausted -->",
                        use_user_account=False
                    )
            
            return True
        
        return False
    
    def _handle_callback(self, payload: bytes):
        """å¤„ç† Actions å›è°ƒ"""
        # éªŒè¯ç­¾å
        signature = self.headers.get("X-Callback-Signature", "")
        if not verify_callback_signature(payload, signature):
            self._send_json(401, {"error": "Invalid callback signature"})
            return
        
        try:
            data = json.loads(payload)
        except json.JSONDecodeError as e:
            self._send_json(400, {"error": f"Invalid JSON: {e}"})
            return
        
        task_id = data.get("task_id", "")
        result = data.get("result", "")
        build_output = data.get("build_output", "")
        
        log(f"Callback received: task={task_id}, result={result}")
        
        if not task_id or not result:
            self._send_json(400, {"error": "Missing task_id or result"})
            return
        
        # è°ƒç”¨å†³ç­–å¼•æ“å¤„ç†
        actions = self.engine.process_callback(
            task_id=task_id,
            result=result,
            build_output=build_output
        )
        
        self._send_json(200, {
            "message": "Callback processed",
            "task_id": task_id,
            "actions_executed": len(actions)
        })


# ==================== ä¸»ç¨‹åº ====================
def main():
    """å¯åŠ¨ä¸­è½¬ç«™"""
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    github_pat = os.environ.get("GITHUB_PAT", "")
    if not github_pat:
        log("ERROR: GITHUB_PAT is required")
        sys.exit(1)
    
    if not WEBHOOK_SECRET:
        log("WARNING: WEBHOOK_SECRET not set")
    
    if not CALLBACK_SECRET:
        log("WARNING: CALLBACK_SECRET not set")
    
    if not PIPELINE_SECRET:
        log("WARNING: PIPELINE_SECRET not set")
    
    # åˆå§‹åŒ–ç»„ä»¶
    store = get_store()
    accounts = get_account_manager()
    github = get_github_client()
    
    # åˆå§‹åŒ– Pipeline Scheduler
    if PIPELINE_REPO_PATH and os.path.exists(PIPELINE_REPO_PATH):
        scheduler = init_scheduler(
            repo_path=PIPELINE_REPO_PATH,
            github_token=github_pat,
            repo_owner=github.repo_owner,
            repo_name=github.repo_name
        )
        log(f"Pipeline Scheduler initialized: {PIPELINE_REPO_PATH}")
    else:
        log(f"WARNING: Pipeline repo not found at {PIPELINE_REPO_PATH}, scheduler disabled")
    
    log("=" * 60)
    log("GitHub Webhook Hub Starting")
    log("=" * 60)
    log(f"Port: {PORT}")
    log(f"Repository: {github.repo_owner}/{github.repo_name}")
    log(f"User accounts: {len(accounts.get_all_accounts_status())}")
    log("")
    log("Endpoints:")
    log("  POST /webhook   - GitHub events")
    log("  POST /callback  - Actions results")
    log("  GET  /status/<id> - Task status")
    log("  GET  /accounts  - Account status")
    log("  GET  /stats     - Statistics")
    log("  GET  /health    - Health check")
    log("")
    log("Pipeline Endpoints:")
    log("  POST /pipeline/ready      - Start pipeline")
    log("  POST /pipeline/cancel/<id> - Cancel pipeline")
    log("  GET  /pipeline/status/<id> - Pipeline status")
    log("  GET  /pipeline/list       - List pipelines")
    log("=" * 60)
    
    server = HTTPServer(("0.0.0.0", PORT), HubHandler)
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        log("Shutting down...")
        server.shutdown()


if __name__ == "__main__":
    main()
