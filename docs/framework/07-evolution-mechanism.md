# è¿›åŒ–æœºåˆ¶ - è‡ªè¿›åŒ–ç¼–ç æ¡†æ¶

## ğŸ§¬ ä»€ä¹ˆåœ¨è¿›åŒ–ï¼Ÿ

### è¿›åŒ–çš„æ ¸å¿ƒï¼šç´¢å¼•é…ç½®

```
âŒ ä¸è¿›åŒ–
â”œâ”€ Agent (LLM) - é¢„è®­ç»ƒæ¨¡å‹ï¼Œèƒ½åŠ›å›ºå®š
â””â”€ Knowledge - çŸ¥è¯†åº“åªå¢ä¸æ”¹

âœ… åœ¨è¿›åŒ–
â”œâ”€ ç‰¹å¾æƒé‡ (feature-weights.json)
â”œâ”€ æ¡ˆä¾‹æ’åº (example-index.json)
â””â”€ æ¨¡å¼ä¼˜å…ˆçº§ (pattern-index.json)
```

**æœ¬è´¨**ï¼šæˆ‘ä»¬ä¸æ”¹å˜Agent"ä¼šä»€ä¹ˆ"ï¼Œè€Œæ˜¯ä¼˜åŒ–"çœ‹ä»€ä¹ˆ"ã€‚

### ä»€ä¹ˆä¸è¿›åŒ–ï¼Ÿ

#### Agentå±‚ï¼šä¸å¯å˜

```
ç†ç”±ï¼š
1. LLMæ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬æ— æ³•ä¿®æ”¹å…¶å‚æ•°
2. å³ä½¿å¾®è°ƒï¼Œæˆæœ¬é«˜ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆ
3. é€šç”¨èƒ½åŠ›å·²è¶³å¤Ÿï¼Œæ— éœ€ç‰¹åŒ–

ç±»æ¯”ï¼š
Agentåƒé€šç”¨è®¡ç®—å™¨ï¼ŒåŠŸèƒ½å›ºå®š
æˆ‘ä»¬ä¼˜åŒ–çš„æ˜¯"è¾“å…¥ä»€ä¹ˆæ•°å­—"ï¼Œè€Œé"å¦‚ä½•è®¡ç®—"
```

#### Knowledgeå±‚ï¼šåªå¢ä¸å‡

```
ç†ç”±ï¼š
1. å†å²æ¡ˆä¾‹æ˜¯å®è´µèµ„äº§ï¼Œä¸åº”åˆ é™¤
2. å³ä½¿æ˜¯"å·®æ¡ˆä¾‹"ä¹Ÿæœ‰å¯¹æ¯”å­¦ä¹ ä»·å€¼
3. çŸ¥è¯†ç§¯ç´¯æ˜¯å•å‘çš„

ç±»æ¯”ï¼š
Knowledgeåƒå›¾ä¹¦é¦†ï¼Œä¹¦ç±åªå¢åŠ ï¼Œä¸é”€æ¯
å³ä½¿æ˜¯è€æ—§çš„ä¹¦ä¹Ÿå¯èƒ½æœ‰å†å²ä»·å€¼
```

## ğŸ“ˆ è¿›åŒ–çš„ä¸‰ä¸ªé˜¶æ®µ

### é˜¶æ®µ1ï¼šæ¢ç´¢æœŸï¼ˆCycle 1-15ï¼‰

**ç‰¹ç‚¹**ï¼š

- ç´¢å¼•æƒé‡éšæœºæˆ–å‡åŒ€åˆ†å¸ƒ
- æŠ«éœ²ç­–ç•¥ä¸ç²¾å‡†
- è´¨é‡æ³¢åŠ¨å¤§
- å¿«é€Ÿè¯•é”™

**æƒé‡å˜åŒ–**ï¼š

```
Cycle 1:  æ‰€æœ‰ç‰¹å¾ 0.50ï¼ˆèµ·ç‚¹ï¼‰
Cycle 5:  å¼€å§‹åˆ†åŒ– 0.48-0.63
Cycle 10: æ˜æ˜¾åˆ†åŒ– 0.45-0.70
Cycle 15: åŒºåˆ†åº¦é«˜ 0.40-0.75
```

**è´¨é‡æ›²çº¿**ï¼š

```
è´¨é‡
0.75 |           â•±â—
     |        â•±  â•±
0.70 |     â—â•±  â•±
     |   â•±  â— â•±
0.65 | â—â•±  â•±  â—
     |â•± â— â•±
0.60 |â—  â•±
     |  â—
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Cycle
      1  3  5  7  9 11 13 15

æ³¢åŠ¨å¤§ï¼Œæ•´ä½“ä¸Šå‡è¶‹åŠ¿
```

**ç­–ç•¥**ï¼š

- å¤§èƒ†å°è¯•ä¸åŒæŠ«éœ²ç­–ç•¥
- å¿«é€Ÿç§¯ç´¯æ ·æœ¬æ•°æ®
- ä¸å¿…è¿½æ±‚å•æ¬¡è´¨é‡æœ€ä¼˜

### é˜¶æ®µ2ï¼šä¼˜åŒ–æœŸï¼ˆCycle 16-40ï¼‰

**ç‰¹ç‚¹**ï¼š

- é«˜ç›¸å…³ç‰¹å¾æƒé‡æŒç»­ä¸Šå‡
- ä½ç›¸å…³ç‰¹å¾æƒé‡ä¸‹é™
- æŠ«éœ²ç­–ç•¥é€æ¸ç²¾å‡†
- è´¨é‡ç¨³æ­¥æå‡

**æƒé‡å˜åŒ–**ï¼š

```
Cycle 20: 0.35-0.80ï¼ˆåŒºåˆ†æ˜æ˜¾ï¼‰
Cycle 30: 0.32-0.88ï¼ˆé«˜ä½åˆ†åŒ–ï¼‰
Cycle 40: 0.30-0.92ï¼ˆæ¥è¿‘ç¨³å®šï¼‰
```

**è´¨é‡æ›²çº¿**ï¼š

```
è´¨é‡
0.85 |                     â—â—
     |                 â—â—â—â—
0.80 |             â—â—â—â—
     |         â—â—â—â—
0.75 |     â—â—â—â—
     | â—â—â—â—
0.70 |â—â—
     |
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Cycle
      16 20 24 28 32 36 40

æ³¢åŠ¨å‡å°ï¼ŒæŒç»­ä¸Šå‡
```

**ç­–ç•¥**ï¼š

- ä¿¡ä»»æ•°æ®ï¼Œè·Ÿéšç›¸å…³æ€§è°ƒæ•´
- æ¯5è½®åˆ†æä¸€æ¬¡
- å…³æ³¨è´¨é‡è¶‹åŠ¿è€Œéå•ç‚¹

### é˜¶æ®µ3ï¼šç¨³å®šæœŸï¼ˆCycle 41+ï¼‰

**ç‰¹ç‚¹**ï¼š

- æƒé‡æ”¶æ•›åˆ°ç¨³å®šåŒºé—´
- æŠ«éœ²ç­–ç•¥æˆç†Ÿ
- è´¨é‡é«˜ä½ç»´æŒ
- å¾®è°ƒä¼˜åŒ–

**æƒé‡ç¨³å®šçŠ¶æ€**ï¼š

```
ç‰¹å¾             æƒé‡    ç½®ä¿¡åº¦
zero_coupling    0.92    0.95
modularity       0.85    0.90
naming           0.65    0.85
error_handling   0.60    0.82
testability      0.58    0.80
performance      0.45    0.75
comments         0.35    0.70
```

**è´¨é‡æ›²çº¿**ï¼š

```
è´¨é‡
0.90 |                 â—â—â—â—â—â—â—
     |             â—â—â—â—
0.85 |         â—â—â—â—
     |     â—â—â—â—
0.80 | â—â—â—â—
     |
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Cycle
      41 45 49 53 57 61 65

é«˜ä½ç¨³å®šï¼Œå°å¹…æ³¢åŠ¨
```

**ç­–ç•¥**ï¼š

- è°¨æ…è°ƒæ•´æƒé‡ï¼ˆå°æ­¥å¹…ï¼‰
- é™ä½å­¦ä¹ ç‡ï¼ˆå¦‚0.2 â†’ 0.1ï¼‰
- å…³æ³¨å¼‚å¸¸å€¼

## ğŸ”¬ ç‰¹å¾ç›¸å…³æ€§åˆ†æ

### è®¡ç®—æ–¹æ³•

#### Pearsonç›¸å…³ç³»æ•°

```python
def calculate_feature_quality_correlation(experiences, feature):
    """
    è®¡ç®—ç‰¹å¾ä¸è´¨é‡çš„Pearsonç›¸å…³ç³»æ•°
    
    Returns:
        ç›¸å…³ç³»æ•° (-1 åˆ° 1)
        - æ¥è¿‘1ï¼šå¼ºæ­£ç›¸å…³ï¼ˆç‰¹å¾å¥½ â†’ è´¨é‡é«˜ï¼‰
        - æ¥è¿‘0ï¼šæ— ç›¸å…³
        - æ¥è¿‘-1ï¼šè´Ÿç›¸å…³ï¼ˆç‰¹å¾å¥½ â†’ è´¨é‡ä½ï¼Œç½•è§ï¼‰
    """
    # æå–ç‰¹å¾åˆ†æ•°å’Œè´¨é‡åˆ†æ•°
    feature_scores = [
        exp['analysis']['features'][feature]
        for exp in experiences
    ]
    quality_scores = [
        exp['output']['quality_score']
        for exp in experiences
    ]
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    n = len(feature_scores)
    mean_f = sum(feature_scores) / n
    mean_q = sum(quality_scores) / n
    
    numerator = sum(
        (feature_scores[i] - mean_f) * (quality_scores[i] - mean_q)
        for i in range(n)
    )
    
    denom_f = sum((f - mean_f) ** 2 for f in feature_scores) ** 0.5
    denom_q = sum((q - mean_q) ** 2 for q in quality_scores) ** 0.5
    
    if denom_f == 0 or denom_q == 0:
        return 0
    
    return numerator / (denom_f * denom_q)
```

#### ç¤ºä¾‹è®¡ç®—

```python
# 10æ¬¡è¿è¡Œæ•°æ®
experiences = [
    {'features': {'zero_coupling': 0.70}, 'quality': 0.65},
    {'features': {'zero_coupling': 0.75}, 'quality': 0.68},
    {'features': {'zero_coupling': 0.80}, 'quality': 0.72},
    {'features': {'zero_coupling': 0.85}, 'quality': 0.75},
    {'features': {'zero_coupling': 0.82}, 'quality': 0.73},
    {'features': {'zero_coupling': 0.88}, 'quality': 0.78},
    {'features': {'zero_coupling': 0.90}, 'quality': 0.80},
    {'features': {'zero_coupling': 0.87}, 'quality': 0.77},
    {'features': {'zero_coupling': 0.92}, 'quality': 0.82},
    {'features': {'zero_coupling': 0.95}, 'quality': 0.85}
]

corr = calculate_correlation(experiences, 'zero_coupling')
# ç»“æœï¼š0.98ï¼ˆå¼ºç›¸å…³ï¼ï¼‰
```

### ç›¸å…³æ€§è§£è¯»

| ç›¸å…³ç³»æ•° | è§£è¯» | æƒé‡è°ƒæ•´ç­–ç•¥ |
|---------|------|-------------|
| 0.90-1.0 | æå¼ºç›¸å…³ | å¤§å¹…æå‡æƒé‡ï¼ˆ+0.15ï¼‰ |
| 0.70-0.90 | å¼ºç›¸å…³ | æå‡æƒé‡ï¼ˆ+0.10ï¼‰ |
| 0.50-0.70 | ä¸­ç­‰ç›¸å…³ | é€‚åº¦æå‡ï¼ˆ+0.05ï¼‰ |
| 0.30-0.50 | å¼±ç›¸å…³ | ä¿æŒæˆ–å¾®é™ï¼ˆÂ±0.02ï¼‰ |
| 0.10-0.30 | æå¼±ç›¸å…³ | é™ä½æƒé‡ï¼ˆ-0.05ï¼‰ |
| 0-0.10 | æ— ç›¸å…³ | å¤§å¹…é™ä½ï¼ˆ-0.10ï¼‰ |

### å¤šç‰¹å¾è”åˆåˆ†æ

```python
def analyze_feature_interactions(experiences):
    """
    åˆ†æç‰¹å¾ä¹‹é—´çš„äº¤äº’æ•ˆåº”
    
    æœ‰æ—¶ä¸¤ä¸ªç‰¹å¾å•ç‹¬ä¸å¼ºï¼Œä½†ç»„åˆåæ•ˆæœå¥½
    """
    features = ['zero_coupling', 'modularity', 'naming']
    
    # è®¡ç®—ä¸¤ä¸¤ç»„åˆçš„ç›¸å…³æ€§
    for f1 in features:
        for f2 in features:
            if f1 >= f2:
                continue
            
            # è®¡ç®—ç»„åˆç‰¹å¾åˆ†æ•°
            combined_scores = [
                (exp['features'][f1] + exp['features'][f2]) / 2
                for exp in experiences
            ]
            quality_scores = [exp['quality'] for exp in experiences]
            
            corr = pearson_correlation(combined_scores, quality_scores)
            
            if corr > 0.90:
                print(f"å‘ç°å¼ºäº¤äº’ï¼š{f1} + {f2} â†’ {corr}")
                # ä¾‹å¦‚ï¼šzero_coupling + modularity ç»„åˆç›¸å…³æ€§0.95
```

## âš™ï¸ æƒé‡æ›´æ–°ç®—æ³•

### æ ‡å‡†æ›´æ–°å…¬å¼

```python
def update_weight(current_weight, correlation, learning_rate=0.20):
    """
    æ ‡å‡†æƒé‡æ›´æ–°ç®—æ³•
    
    Args:
        current_weight: å½“å‰æƒé‡ (0-1)
        correlation: ç›¸å…³ç³»æ•° (-1 åˆ° 1)
        learning_rate: å­¦ä¹ ç‡ (0-1)
    
    Returns:
        new_weight: æ›´æ–°åæƒé‡ (0.20-0.95)
    """
    # å°†ç›¸å…³ç³»æ•°æ˜ å°„åˆ° [0, 1]
    normalized_corr = (correlation + 1) / 2
    
    # åŠ æƒç§»åŠ¨å¹³å‡
    new_weight = (
        current_weight * (1 - learning_rate) +
        normalized_corr * learning_rate
    )
    
    # é™åˆ¶èŒƒå›´
    MIN_WEIGHT = 0.20  # ä¸å®Œå…¨å¿½ç•¥ä»»ä½•ç‰¹å¾
    MAX_WEIGHT = 0.95  # ä¸è¿‡åº¦ä¾èµ–å•ä¸€ç‰¹å¾
    new_weight = max(MIN_WEIGHT, min(MAX_WEIGHT, new_weight))
    
    return round(new_weight, 2)
```

### è‡ªé€‚åº”å­¦ä¹ ç‡

```python
def adaptive_learning_rate(cycle, variance):
    """
    æ ¹æ®å¾ªç¯è½®æ•°å’Œè´¨é‡æ–¹å·®åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
    
    æ—©æœŸï¼šé«˜å­¦ä¹ ç‡ï¼Œå¿«é€Ÿæ¢ç´¢
    åæœŸï¼šä½å­¦ä¹ ç‡ï¼Œç²¾ç»†è°ƒä¼˜
    """
    if cycle < 10:
        base_rate = 0.30  # æ¢ç´¢æœŸï¼Œå¤§æ­¥è°ƒæ•´
    elif cycle < 30:
        base_rate = 0.20  # ä¼˜åŒ–æœŸï¼Œç¨³æ­¥è°ƒæ•´
    else:
        base_rate = 0.10  # ç¨³å®šæœŸï¼Œå°æ­¥å¾®è°ƒ
    
    # å¦‚æœè´¨é‡æ³¢åŠ¨å¤§ï¼Œé™ä½å­¦ä¹ ç‡
    if variance > 0.05:
        return base_rate * 0.5
    
    return base_rate
```

### åŠ¨é‡æ›´æ–°ï¼ˆå¯é€‰ï¼‰

```python
def momentum_update(current_weight, correlation, momentum=0.9):
    """
    å¼•å…¥åŠ¨é‡ï¼Œé¿å…æƒé‡å‰§çƒˆéœ‡è¡
    
    ç±»ä¼¼æ·±åº¦å­¦ä¹ ä¸­çš„SGD+Momentum
    """
    # ä¿å­˜å†å²æ¢¯åº¦
    if not hasattr(momentum_update, 'velocity'):
        momentum_update.velocity = {}
    
    feature = 'current_feature'  # ç¤ºä¾‹
    
    # è®¡ç®—æ¢¯åº¦ï¼ˆç›®æ ‡æƒé‡ - å½“å‰æƒé‡ï¼‰
    normalized_corr = (correlation + 1) / 2
    gradient = normalized_corr - current_weight
    
    # æ›´æ–°é€Ÿåº¦
    if feature not in momentum_update.velocity:
        momentum_update.velocity[feature] = 0
    
    velocity = (
        momentum * momentum_update.velocity[feature] +
        (1 - momentum) * gradient
    )
    momentum_update.velocity[feature] = velocity
    
    # åº”ç”¨é€Ÿåº¦
    new_weight = current_weight + 0.20 * velocity
    
    return max(0.20, min(0.95, new_weight))
```

## ğŸ“š æ¡ˆä¾‹åº“ç®¡ç†

### æ¡ˆä¾‹åˆ†çº§

```python
def classify_example(code, quality_score):
    """
    æ ¹æ®è´¨é‡åˆ†æ•°å¯¹æ¡ˆä¾‹åˆ†çº§
    """
    if quality_score >= 0.85:
        return 'excellent'
    elif quality_score >= 0.70:
        return 'good'
    elif quality_score >= 0.55:
        return 'average'
    else:
        return 'poor'
```

### æ¡ˆä¾‹è‡ªåŠ¨å½’æ¡£

```python
def archive_example(module_file, quality_score):
    """
    å°†ç”Ÿäº§çš„æ¨¡å—å½’æ¡£åˆ°æ¡ˆä¾‹åº“
    """
    classification = classify_example(code, quality_score)
    
    if classification in ['excellent', 'good', 'average']:
        # å¤åˆ¶åˆ°æ¡ˆä¾‹åº“
        target_dir = f'knowledge/examples/{classification}/'
        shutil.copy(module_file, target_dir)
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'file': os.path.basename(module_file),
            'quality_score': quality_score,
            'features': extract_feature_scores(code),
            'created_at': now(),
            'usage_count': 0,
            'avg_result_quality': quality_score
        }
        write_json(f'{target_dir}/{filename}.meta.json', metadata)
        
        # æ›´æ–°æ¡ˆä¾‹ç´¢å¼•
        update_example_index(filename, classification, metadata)
```

### æ¡ˆä¾‹æ·˜æ±°æœºåˆ¶

```python
def cleanup_poor_examples():
    """
    å®šæœŸæ¸…ç†æ•ˆæœå·®çš„æ¡ˆä¾‹ï¼ˆå¯é€‰ï¼‰
    
    ä¿ç•™è§„åˆ™ï¼š
    1. excellentæ¡ˆä¾‹ï¼šæ°¸ä¹…ä¿ç•™
    2. averageæ¡ˆä¾‹ï¼šä½¿ç”¨æ¬¡æ•°>10æˆ–æ•ˆæœå¥½
    3. pooræ¡ˆä¾‹ï¼šä¸å½’æ¡£
    """
    example_index = read_json('.state/indices/example-index.json')
    
    for feature in example_index['by_feature']:
        average_examples = example_index['by_feature'][feature]['average']
        
        # ç­›é€‰ä½æ•ˆæ¡ˆä¾‹
        to_remove = [
            ex for ex in average_examples
            if ex['usage_count'] < 5 and ex['avg_result_quality'] < 0.65
        ]
        
        for ex in to_remove:
            print(f"ç§»é™¤ä½æ•ˆæ¡ˆä¾‹ï¼š{ex['file']}")
            # ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•ï¼Œè€Œéåˆ é™¤
            shutil.move(
                f'knowledge/examples/average/{ex["file"]}',
                f'knowledge/examples/archived/{ex["file"]}'
            )
```

## ğŸ“Š å¦‚ä½•æµ‹é‡è¿›åŒ–

### å…³é”®æŒ‡æ ‡

#### 1. è´¨é‡è¶‹åŠ¿

```python
def measure_quality_trend(history):
    """
    æµ‹é‡è´¨é‡æ”¹è¿›è¶‹åŠ¿
    
    Returns:
        - trend: 'improving' | 'declining' | 'stable'
        - rate: æ”¹è¿›é€Ÿç‡ï¼ˆåˆ†æ•°/è½®ï¼‰
    """
    if len(history) < 10:
        return {'trend': 'insufficient_data', 'rate': 0}
    
    # è®¡ç®—çº¿æ€§å›å½’æ–œç‡
    x = list(range(len(history)))
    y = [h['quality_score'] for h in history]
    
    slope = linear_regression_slope(x, y)
    
    if slope > 0.01:
        return {'trend': 'improving', 'rate': slope}
    elif slope < -0.01:
        return {'trend': 'declining', 'rate': slope}
    else:
        return {'trend': 'stable', 'rate': slope}
```

#### 2. æƒé‡æ”¶æ•›åº¦

```python
def measure_convergence(weight_history):
    """
    æµ‹é‡æƒé‡æ˜¯å¦æ”¶æ•›
    
    Returns:
        - converged: bool
        - convergence_score: 0-1ï¼ˆè¶Šé«˜è¶Šæ”¶æ•›ï¼‰
    """
    if len(weight_history) < 5:
        return {'converged': False, 'score': 0}
    
    # è®¡ç®—æœ€è¿‘5è½®çš„æƒé‡æ–¹å·®
    recent_weights = weight_history[-5:]
    variance = calculate_variance(recent_weights)
    
    # æ–¹å·®å° â†’ æ”¶æ•›åº¦é«˜
    convergence_score = 1.0 - min(1.0, variance * 20)
    
    converged = variance < 0.05  # é˜ˆå€¼
    
    return {
        'converged': converged,
        'score': convergence_score,
        'variance': variance
    }
```

#### 3. æŠ«éœ²æ•ˆç‡

```python
def measure_disclosure_efficiency(traces):
    """
    æµ‹é‡æŠ«éœ²çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç²¾å‡†
    
    æŠ«éœ²æ•ˆç‡ = è´¨é‡æå‡ / æŠ«éœ²çš„tokenæ•°
    """
    efficiencies = []
    
    for trace in traces:
        disclosed_tokens = trace['disclosed_tokens']
        quality = trace['quality_score']
        
        # å‡è®¾åŸºå‡†è´¨é‡0.60ï¼ˆæ— æŠ«éœ²ï¼‰
        baseline_quality = 0.60
        improvement = quality - baseline_quality
        
        efficiency = improvement / (disclosed_tokens / 1000)
        efficiencies.append(efficiency)
    
    return {
        'avg_efficiency': mean(efficiencies),
        'trend': 'improving' if efficiencies[-1] > efficiencies[0] else 'declining'
    }
```

### è¿›åŒ–ä»ªè¡¨ç›˜

```
=== è¿›åŒ–çŠ¶æ€ä»ªè¡¨ç›˜ (Cycle 50) ===

è´¨é‡è¶‹åŠ¿ï¼š
  å½“å‰ï¼š0.88 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â†‘ (improving)
  é€Ÿç‡ï¼š+0.015 / cycle
  ç¨³å®šæ€§ï¼šæ–¹å·® 0.011 (æ”¶æ•›)

æƒé‡æ”¶æ•›ï¼š
  çŠ¶æ€ï¼šå·²æ”¶æ•› âœ“
  æ”¶æ•›åˆ†æ•°ï¼š0.92
  æœ€å¤§å˜åŒ–ï¼š<0.03

ç‰¹å¾æƒé‡åˆ†å¸ƒï¼š
  zero_coupling:  0.92 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  modularity:     0.85 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  naming:         0.65 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  error_handling: 0.60 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  testability:    0.58 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  performance:    0.45 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  comments:       0.35 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

æŠ«éœ²æ•ˆç‡ï¼š
  å¹³å‡æ•ˆç‡ï¼š0.12 è´¨é‡/1K tokens
  è¶‹åŠ¿ï¼šimproving (+15% vs åˆæœŸ)

æ¡ˆä¾‹åº“ï¼š
  æ€»æ•°ï¼š138
  ä¼˜ç§€ï¼š85 (62%)
  è‰¯å¥½ï¼š40 (29%)
  æ™®é€šï¼š13 (9%)

æ¨èè¡ŒåŠ¨ï¼š
  âœ“ ç³»ç»Ÿå·²ç¨³å®šï¼Œå¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨
  - ç»§ç»­ç»´æŒå­¦ä¹ å¾ªç¯ï¼ˆä½é¢‘ï¼Œå¦‚æ¯20è½®ï¼‰
  - å…³æ³¨å¼‚å¸¸è´¨é‡ä¸‹é™
```

## ğŸ”® è¿›åŒ–çš„é•¿æœŸå±•æœ›

### æŒç»­å­¦ä¹ 

```
å³ä½¿ç³»ç»Ÿç¨³å®šåï¼Œä¹Ÿåº”ä¿æŒå­¦ä¹ ï¼š

1. æ–°APIå‘å¸ƒæ—¶
   â†’ æ·»åŠ æ–°èƒ½åŠ›ç‚¹
   â†’ ç”Ÿäº§æ–°ç§¯æœ¨
   â†’ æƒé‡å¯èƒ½å¾®è°ƒ

2. éœ€æ±‚æ¨¡å¼å˜åŒ–æ—¶
   â†’ æŸäº›ç‰¹å¾å˜å¾—æ›´é‡è¦
   â†’ æƒé‡è‡ªç„¶è°ƒæ•´

3. æŠ€æœ¯æ ˆå‡çº§æ—¶
   â†’ å¯èƒ½éœ€è¦éƒ¨åˆ†é‡ç½®
   â†’ ä¿ç•™æ ¸å¿ƒæƒé‡ç»“æ„
```

### è·¨é¡¹ç›®è¿ç§»

```
æƒé‡å¯åœ¨ç±»ä¼¼é¡¹ç›®é—´è¿ç§»ï¼š

é¡¹ç›®A (UEFNæ¸¸æˆ1)ï¼š
  æƒé‡ï¼š{zero_coupling: 0.92, modularity: 0.85, ...}
  
é¡¹ç›®B (UEFNæ¸¸æˆ2ï¼Œæ–°é¡¹ç›®)ï¼š
  åˆå§‹æƒé‡ï¼šä»é¡¹ç›®Aå¤åˆ¶
  â†’ å¿«é€Ÿå¯åŠ¨ï¼ˆè·³è¿‡æ¢ç´¢æœŸï¼‰
  â†’ å¾®è°ƒé€‚åº”æ–°é¡¹ç›®ç‰¹ç‚¹
  
èŠ‚çœæ—¶é—´ï¼š~15è½®å¾ªç¯ï¼ˆçº¦40%æ—¶é—´ï¼‰
```

### é›†ä½“æ™ºæ…§

```
å›¢é˜Ÿå…±äº«æƒé‡é…ç½®ï¼š

å›¢é˜Ÿæˆå‘˜Aï¼š
  ä¸“æ³¨æˆ˜æ–—ç³»ç»Ÿï¼Œä¼˜åŒ–combatç›¸å…³æƒé‡

å›¢é˜Ÿæˆå‘˜Bï¼š
  ä¸“æ³¨UIç³»ç»Ÿï¼Œä¼˜åŒ–uiç›¸å…³æƒé‡

å®šæœŸåˆå¹¶ï¼š
  ç»¼åˆå„æˆå‘˜çš„æƒé‡å­¦ä¹ æˆæœ
  â†’ å›¢é˜Ÿçº§æœ€ä¼˜æƒé‡é…ç½®
```

## ğŸ“– ä¸‹ä¸€æ­¥

- **å¼€å§‹å®æ–½** â†’ [08-implementation-guide.md](./08-implementation-guide.md)
- **å›é¡¾æ¶æ„** â†’ [01-architecture.md](./01-architecture.md)

---

**è¿”å›** â†’ [æ¡†æ¶æ–‡æ¡£é¦–é¡µ](./README.md)
