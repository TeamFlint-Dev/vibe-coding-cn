## H005: 解释性评论提升用户对 AI 自动化的信任

**状态**: `proposed`

**提出者**: workflow-case-study Agent (Run #4)  
**提出日期**: 2026-01-10  
**最后更新**: 2026-01-10

---

### 观察依据

在分析 `issue-triage-agent` 工作流时，发现其设计中明确要求：

> "After adding the label to an issue, mention the issue author in a comment explaining why the label was added."

这不仅仅是添加标签，还要：
1. @mention 作者（建立直接沟通）
2. 解释理由（透明化决策过程）

对比 `issue-classifier` 没有类似要求，只是静默添加标签。

---

### 猜想陈述

**我认为**: 当 AI Agent 执行自动化操作后附加「解释性评论」（说明做了什么、为什么这样做），会显著提升用户对自动化系统的信任度和接受度。

---

### 验证标准

**证实条件** (满足任一):
- [ ] 找到 3+ 个使用解释性评论的工作流，且有正面用户反馈
- [ ] 找到对比案例：同类工作流有/无解释，用户满意度差异明显
- [ ] 找到官方文档或设计指南推荐此模式

**证伪条件** (满足任一):
- [ ] 找到使用解释性评论但用户仍不信任/不满意的案例
- [ ] 发现解释性评论增加的噪音导致用户负面反馈
- [ ] 官方明确建议避免此模式

**边界探索**:
- 什么类型的操作需要解释？（高影响/低影响）
- 解释应该多详细？（一句话 vs 详细分析）
- 所有自动化都需要解释吗？还是只有「分类/判断」类型需要？

---

### 证据链

| 日期 | 贡献者 | 类型 | 摘要 | 权重 |
|------|--------|------|------|------|
| 2026-01-10 | workflow-case-study #4 | support | issue-triage-agent 要求解释标签添加理由 | +1 |

**证据统计**: 支持 1 / 反例 0 / 边界 0

---

### 当前结论

初步观察支持此猜想，但证据不足。需要：
1. 寻找更多使用/不使用解释性评论的工作流对比
2. 寻找用户反馈数据
3. 验证此模式在不同场景下的适用性

---

### 关联猜想

| 关系 | 猜想 | 说明 |
|------|------|------|
| - | - | 暂无直接关联猜想 |

---

### 理论背景（待探索）

可能相关的理论：
- **可解释 AI (XAI)**: AI 系统应提供决策解释
- **信任建立理论**: 透明度是建立信任的关键因素
- **用户中心设计**: 让用户感觉被尊重和知情

---

### 备注

此猜想可能催生新的 UX 设计模式：**Author Notification Pattern**

配置建议（待验证）：
```yaml
safe-outputs:
  add-labels: ...
  add-comment:
    # 建议在 prompt 中要求附加解释
```

Prompt 建议片段：
> "After performing the action, comment to explain your reasoning so the user understands the decision."
